{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSE474/574 - Programming Assignment 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1 - Sentiment Analysis\n",
    "\n",
    "In the code provided below, you need to add code wherever specified by `TODO:`. \n",
    "\n",
    "> You will be using a Python collection class - `Counter` to maintain the word counts. \n",
    "\n",
    "> See https://docs.python.org/2/library/collections.html for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data files \n",
    "g = open('reviews.txt','r') # What we know!\n",
    "reviews_all = list(map(lambda x:x[:-1],g.readlines()))\n",
    "g.close()\n",
    "g = open('labels.txt','r') # What we WANT to know!\n",
    "sentiments_all = list(map(lambda x:x[:-1].upper(),g.readlines()))\n",
    "g.close()\n",
    "\n",
    "# load vocabulary\n",
    "g = open('vocab.txt','r')\n",
    "vocab = [s.strip() for s in g.readlines()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is a set of 25000 movie reviews, along with a `POSITIVE` or `NEGATIVE` sentiment label assigned to the review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A POSITIVE review:\n",
      "bromwell high is a cartoon comedy . it ran at the same time as some other programs about school life  such as  teachers  . my   years in the teaching profession lead me to believe that bromwell high  s satire is much closer to reality than is  teachers  . the scramble to survive financially  the insightful students who can see right through their pathetic teachers  pomp  the pettiness of the whole situation  all remind me of the schools i knew and their students . when i saw the episode in which a student repeatedly tried to burn down the school  i immediately recalled . . . . . . . . . at . . . . . . . . . . high . a classic line inspector i  m here to sack one of your teachers . student welcome to bromwell high . i expect that many adults of my age think that bromwell high is far fetched . what a pity that it isn  t   \n",
      "\n",
      "A NEGATIVE review:\n",
      "story of a man who has unnatural feelings for a pig . starts out with a opening scene that is a terrific example of absurd comedy . a formal orchestra audience is turned into an insane  violent mob by the crazy chantings of it  s singers . unfortunately it stays absurd the whole time with no general narrative eventually making it just too off putting . even those from the era should be turned off . the cryptic dialogue would make shakespeare seem easy to a third grader . on a technical level it  s better than you might think with some good cinematography by future great vilmos zsigmond . future stars sally kirkland and frederic forrest can be seen briefly .  \n"
     ]
    }
   ],
   "source": [
    "# Check out sample reviews\n",
    "print('A {} review:'.format(sentiments_all[0]))\n",
    "print(reviews_all[0])\n",
    "print('\\nA {} review:'.format(sentiments_all[1]))\n",
    "print(reviews_all[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into training and test data\n",
    "reviews_train,reviews_test = reviews_all[0:24000],reviews_all[24000:]\n",
    "sentiments_train,sentiments_test = sentiments_all[0:24000],sentiments_all[24000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# maintain Counter objects to store positive, negative and total counts for\n",
    "# all the words present in the positive, negative and total reviews.\n",
    "positive_word_count = Counter()\n",
    "negative_word_count = Counter()\n",
    "total_counts = Counter()\n",
    "# TODO: Loop over all the words in the vocabulary\n",
    "# and increment the counts in the appropriate counter objects\n",
    "# based on the training data\n",
    "for r,n in zip(reviews_train,sentiments_train):\n",
    "    \n",
    "    if n==\"NEGATIVE\":\n",
    "        r = r.split()\n",
    "        for r1 in r:\n",
    "            if (len(r1)>1 or r1==\"a\"):\n",
    "                negative_word_count[r1] = negative_word_count[r1]+1\n",
    "                total_counts[r1] = total_counts[r1] +1\n",
    "\n",
    "    else:\n",
    "        r = r.split()\n",
    "        for r2 in r:\n",
    "            if (len(r2)>1 or r2 ==\"a\"):\n",
    "                positive_word_count[r2] = positive_word_count[r2]+1\n",
    "                total_counts[r2] = total_counts[r2] +1\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# maintain a Counter object to store positive to negative ratios \n",
    "pos_neg_ratios = Counter()\n",
    "\n",
    "# Calculate the ratios of positive and negative uses of the most common words\n",
    "# Consider words to be \"common\" if they've been used at least 100 times\n",
    "for term,cnt in list(total_counts.most_common()):\n",
    "    if(cnt > 100):\n",
    "        # TODO: Code for calculating the ratios (remove the next line)\n",
    "        \n",
    "        if(positive_word_count[term]==0 or negative_word_count[term] ==0):\n",
    "            pos_neg_ratios[term] = (positive_word_count[term]+1)/(negative_word_count[term]+1)\n",
    "        else:\n",
    "            pos_neg_ratios[term] = positive_word_count[term]/negative_word_count[term]\n",
    "#print(t\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pos-to-neg ratio for 'the' = 1.0618582280413789\n",
      "Pos-to-neg ratio for 'amazing' = 4.031496062992126\n",
      "Pos-to-neg ratio for 'terrible' = 0.17256637168141592\n"
     ]
    }
   ],
   "source": [
    "print(\"Pos-to-neg ratio for 'the' = {}\".format(pos_neg_ratios[\"the\"]))\n",
    "print(\"Pos-to-neg ratio for 'amazing' = {}\".format(pos_neg_ratios[\"amazing\"]))\n",
    "print(\"Pos-to-neg ratio for 'terrible' = {}\".format(pos_neg_ratios[\"terrible\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take a log of the ratio\n",
    "for word,ratio in pos_neg_ratios.most_common():\n",
    "    pos_neg_ratios[word] = np.log(ratio)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# visualize the distribution of the log-ratio scores\n",
    "scores = np.array(list(pos_neg_ratios.values()))\n",
    "vocab_selected = list(pos_neg_ratios.keys())\n",
    "\n",
    "h = plt.hist(scores,bins=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above histogram should give you an idea about the distribution of the scores.\n",
    "\n",
    "Notice how the scores are distributed around 0. A word with score 0 can be considered as `neutral`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "realize\n",
      "hands\n",
      "extreme\n",
      "beat\n",
      "onto\n",
      "psycho\n",
      "test\n",
      "obsessed\n",
      "choose\n",
      "speech\n"
     ]
    }
   ],
   "source": [
    "# Print few words with neutral score\n",
    "for ind in np.where(scores == 0)[0][0:10]:\n",
    "    print(vocab_selected[ind])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**APPROACH 1** Implement a simple non-machine learning that only uses the log-ratios to determine if a review is positive or negative. This function will be applied to the test data to calculate the accuracy of the model. \n",
    "\n",
    "_See the assignment document for hints._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nonml_classifier(review,pos_neg_ratios):\n",
    "    '''\n",
    "    Function that determines the sentiment for a given review.\n",
    "    \n",
    "    Inputs:\n",
    "      review - A text containing a movie review\n",
    "      pos_neg_ratios - A Counter object containing frequent words\n",
    "                       and corresponding log positive-negative ratio\n",
    "    Return:\n",
    "      sentiment - 'NEGATIVE' or 'POSITIVE'\n",
    "    '''\n",
    "    # TODO: Implement the algorithm here. Change the next line.\n",
    "    sentiment = 'POSITIVE'\n",
    "    countp =0\n",
    "    list1 = review.split()\n",
    "    \n",
    "    for l in list1:    \n",
    "        countp = countp+pos_neg_ratios[l]\n",
    "        if(countp>0):\n",
    "            sentiment = 'POSITIVE'\n",
    "        else:\n",
    "            sentiment = 'NEGATIVE'\n",
    "    return sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the model = 0.768\n"
     ]
    }
   ],
   "source": [
    "predictions_test = []\n",
    "for r in reviews_test:\n",
    "    l = nonml_classifier(r,pos_neg_ratios)\n",
    "    predictions_test.append(l)\n",
    "\n",
    "correct = 0\n",
    "for l,p in zip(sentiments_test,predictions_test):\n",
    "    if l == p:\n",
    "        correct = correct + 1\n",
    "print('Accuracy of the model = {}'.format(correct/len(sentiments_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Approach 2** Implement a neural network for sentiment classification. \n",
    "\n",
    "> ### System Configuration\n",
    "This part requires you to use a computer with `tensorflow` library installed. More information is available here - https://www.tensorflow.org.\n",
    "`\n",
    "You are allowed to implement the project on your personal computers using `Python 3.4 or above. You will need `numpy` and `scipy` libraries. If you need to use departmental resources, you can use **metallica.cse.buffalo.edu**, which has `Python 3.4.3` and the required libraries installed. \n",
    "\n",
    "> Students attempting to use the `tensorflow` library have two options: \n",
    "1. Install `tensorflow` on personal machines. Detailed installation information is here - https://www.tensorflow.org/. Note that, since `tensorflow` is a relatively new library, you might encounter installation issues depending on your OS and other library versions. We will not be providing any detailed support regarding `tensorflow` installation. If issues persist, we recommend using option 2. \n",
    "2. Use **metallica.cse.buffalo.edu**. If you are registered into the class, you should have an account on that server. The server already has Python 3.4.3 and TensorFlow 0.12.1 installed. Please use /util/bin/python for Python 3. \n",
    "3. To maintain a ssh connection for a long-running task on a remote machine, use tools like `screen`. For more information: https://linuxize.com/post/how-to-use-linux-screen/ \n",
    "4. For running jupyter-notebook over a remote machine find information on: https://fizzylogic.nl/2017/11/06/edit-jupyter-notebooks-over-ssh/\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_input_vector(review,word2index):\n",
    "    '''\n",
    "    Function to count how many times each word is used in the given review,\n",
    "    # and then store those counts at the appropriate indices inside x.\n",
    "    '''\n",
    "    vocab_size = len(word2index)\n",
    "    x = np.zeros((1, vocab_size))\n",
    "    for w in review.split(' '):\n",
    "        if w in word2index.keys():\n",
    "            x[0][word2index[w]] += 1\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_ignore_words(pos_neg_ratios):\n",
    "    ''' \n",
    "    Function to identify words to ignore from the vocabulary\n",
    "    '''\n",
    "    ignore_words = []\n",
    "    # TODO: Complete the implementation of find_ignore_words\n",
    "    variance = 0\n",
    "    listvalues = list(pos_neg_ratios.values())\n",
    "    mean = np.mean(listvalues)\n",
    "    n = len(listvalues)\n",
    "    for v in listvalues:\n",
    "        variance = variance+((v-mean)**2)/n\n",
    "    SD = np.sqrt(variance)\n",
    "    for k in pos_neg_ratios:\n",
    "        if(pos_neg_ratios[k]< 0.19*SD and pos_neg_ratios[k]> -0.19*SD):\n",
    "            ignore_words.append(k) \n",
    "    \n",
    "    '''\n",
    "    ignore_words = []\n",
    "    for w in pos_neg_ratios:\n",
    "        if pos_neg_ratios[w]<0.5 and pos_neg_ratios[w]>-0.5:\n",
    "            ignore_words.append(w)\n",
    "    '''\n",
    "    return ignore_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a word2index mapping from word to an integer index\n",
    "word2index = {}\n",
    "ignore_words = find_ignore_words(pos_neg_ratios)\n",
    "vocab_selected = list(set(vocab_selected).difference(set(ignore_words)))\n",
    "for i,word in enumerate(vocab_selected):\n",
    "    if word not in ignore_words:\n",
    "        word2index[word] = i\n",
    "vocab_size = len(word2index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate .hdf5 files from the processed data\n",
    "Given that the data is moderately large sized, the `hdf5` file format provides a more efficient file representation for further processing. See [here](https://anaconda.org/anaconda/hdf5) for more details and installation instructions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the script once to generate the file \n",
    "# delete the exiting 'data1.hdf5' file before running it again to avoid error \n",
    "labels_train = np.zeros((len(sentiments_train), 2), dtype=int)\n",
    "labels_test = np.zeros((len(sentiments_test), 2), dtype=int)\n",
    "\n",
    "with h5py.File('data1.hdf5', 'w') as hf:\n",
    "    hf.create_dataset('data_train', (labels_train.shape[0], vocab_size), np.int16)\n",
    "    hf.create_dataset('data_test', (labels_test.shape[0], vocab_size), np.int16)\n",
    "    # create training data\n",
    "    for i,(r,l) in enumerate(zip(reviews_train, sentiments_train)):\n",
    "        hf[\"data_train\"][i] = create_input_vector(r,word2index)\n",
    "        # one-hot encoding\n",
    "        if l == 'NEGATIVE':\n",
    "            labels_train[i, 0] = 1\n",
    "        else:\n",
    "            labels_train[i, 1] = 1\n",
    "    # create test data\n",
    "    for i,(r,l) in enumerate(zip(reviews_test, sentiments_test)):\n",
    "        hf[\"data_test\"][i] = create_input_vector(r,word2index)\n",
    "        # one-hot encoding\n",
    "        if l == 'NEGATIVE':\n",
    "            labels_test[i, 0] = 1\n",
    "        else:\n",
    "            labels_test[i, 1] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "tf.compat.v1.random.set_random_seed(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters of the network\n",
    "learning_rate = 0.01\n",
    "batch_size = 400\n",
    "num_epochs = 50\n",
    "n_input = vocab_size\n",
    "n_classes = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#1layer architecture\n",
    "X = tf.placeholder(\"float\", [None, n_input])\n",
    "Y = tf.placeholder(\"float\", [None, n_classes])\n",
    "# Define weights and biases in Tensorflow according to the parameters set above\n",
    "n_hidden_1 = 10  # 1st layer number of neurons\n",
    "weights = {\n",
    "\t'h1': tf.Variable(tf.random_normal([n_input, n_hidden_1])),\n",
    "\t'out1': tf.Variable(tf.random_normal([n_hidden_1, n_classes]))\n",
    "}\n",
    "biases = {\n",
    "\t'b1': tf.Variable(tf.random_normal([n_hidden_1])),\n",
    "    'out2': tf.Variable(tf.random_normal([n_classes]))\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n#2 layer architecture\\nX = tf.placeholder(\"float\", [None, n_input])\\nY = tf.placeholder(\"float\", [None, n_classes])\\n#print(n_hidden_1)\\nprint(Y)\\n# Define weights and biases in Tensorflow according to the parameters set above\\nn_hidden_1 = 50  # 1st layer number of neurons\\nn_hidden_2 = 40\\nweights = {\\n\\t\\'h1\\': tf.Variable(tf.random_normal([n_input, n_hidden_1])),\\n    \\'h2\\': tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2])),\\n\\t\\'out1\\': tf.Variable(tf.random_normal([n_hidden_2, n_classes]))\\n}\\nbiases = {\\n\\t\\'b1\\': tf.Variable(tf.random_normal([n_hidden_1])),\\n\\t\\'b2\\': tf.Variable(tf.random_normal([n_hidden_2])),\\n\\t\\'out2\\': tf.Variable(tf.random_normal([n_classes]))\\n}\\nprint(weights[\\'h1\\'])\\n#print(weights[\\'h2\\'])\\nprint(weights[\\'out1\\'])\\n'"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "#2 layer architecture\n",
    "X = tf.placeholder(\"float\", [None, n_input])\n",
    "Y = tf.placeholder(\"float\", [None, n_classes])\n",
    "#print(n_hidden_1)\n",
    "print(Y)\n",
    "# Define weights and biases in Tensorflow according to the parameters set above\n",
    "n_hidden_1 = 50  # 1st layer number of neurons\n",
    "n_hidden_2 = 40\n",
    "weights = {\n",
    "\t'h1': tf.Variable(tf.random_normal([n_input, n_hidden_1])),\n",
    "    'h2': tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2])),\n",
    "\t'out1': tf.Variable(tf.random_normal([n_hidden_2, n_classes]))\n",
    "}\n",
    "biases = {\n",
    "\t'b1': tf.Variable(tf.random_normal([n_hidden_1])),\n",
    "\t'b2': tf.Variable(tf.random_normal([n_hidden_2])),\n",
    "\t'out2': tf.Variable(tf.random_normal([n_classes]))\n",
    "}\n",
    "print(weights['h1'])\n",
    "#print(weights['h2'])\n",
    "print(weights['out1'])\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n#3 layer architecture\\nX = tf.placeholder(\"float\", [None, n_input])\\nY = tf.placeholder(\"float\", [None, n_classes])\\n#print(n_hidden_1)\\nprint(Y)\\n# Define weights and biases in Tensorflow according to the parameters set above\\nn_hidden_1 = 20  # 1st layer number of neurons\\nn_hidden_2 = 20\\nn_hidden_3 = 10\\nweights = {\\n\\t\\'h1\\': tf.Variable(tf.random_normal([n_input, n_hidden_1])),\\n    \\'h2\\': tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2])),\\n    \\'h3\\': tf.Variable(tf.random_normal([n_hidden_2, n_hidden_3])),\\n\\t\\'out1\\': tf.Variable(tf.random_normal([n_hidden_3, n_classes]))\\n}\\nbiases = {\\n\\t\\'b1\\': tf.Variable(tf.random_normal([n_hidden_1])),\\n\\t\\'b2\\': tf.Variable(tf.random_normal([n_hidden_2])),\\n    \\'b3\\': tf.Variable(tf.random_normal([n_hidden_3])),\\n\\t\\'out2\\': tf.Variable(tf.random_normal([n_classes]))\\n}\\n'"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "#3 layer architecture\n",
    "X = tf.placeholder(\"float\", [None, n_input])\n",
    "Y = tf.placeholder(\"float\", [None, n_classes])\n",
    "#print(n_hidden_1)\n",
    "print(Y)\n",
    "# Define weights and biases in Tensorflow according to the parameters set above\n",
    "n_hidden_1 = 20  # 1st layer number of neurons\n",
    "n_hidden_2 = 20\n",
    "n_hidden_3 = 10\n",
    "weights = {\n",
    "\t'h1': tf.Variable(tf.random_normal([n_input, n_hidden_1])),\n",
    "    'h2': tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2])),\n",
    "    'h3': tf.Variable(tf.random_normal([n_hidden_2, n_hidden_3])),\n",
    "\t'out1': tf.Variable(tf.random_normal([n_hidden_3, n_classes]))\n",
    "}\n",
    "biases = {\n",
    "\t'b1': tf.Variable(tf.random_normal([n_hidden_1])),\n",
    "\t'b2': tf.Variable(tf.random_normal([n_hidden_2])),\n",
    "    'b3': tf.Variable(tf.random_normal([n_hidden_3])),\n",
    "\t'out2': tf.Variable(tf.random_normal([n_classes]))\n",
    "}\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n#5 layer architecture\\nX = tf.placeholder(\"float\", [None, n_input])\\nY = tf.placeholder(\"float\", [None, n_classes])\\n#print(n_hidden_1)\\nprint(Y)\\n# Define weights and biases in Tensorflow according to the parameters set above\\nn_hidden_1 = 35  # 1st layer number of neurons\\nn_hidden_2 = 30\\nn_hidden_3 = 25\\nn_hidden_4 = 20\\nn_hidden_5 = 15\\nweights = {\\n\\t\\'h1\\': tf.Variable(tf.random_normal([n_input, n_hidden_1])),\\n    \\'h2\\': tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2])),\\n    \\'h3\\': tf.Variable(tf.random_normal([n_hidden_2, n_hidden_3])),\\n    \\'h4\\': tf.Variable(tf.random_normal([n_hidden_3, n_hidden_4])),\\n    \\'h5\\': tf.Variable(tf.random_normal([n_hidden_4, n_hidden_5])),\\n\\t\\'out1\\': tf.Variable(tf.random_normal([n_hidden_5, n_classes]))\\n}\\nbiases = {\\n\\t\\'b1\\': tf.Variable(tf.random_normal([n_hidden_1])),\\n\\t\\'b2\\': tf.Variable(tf.random_normal([n_hidden_2])),\\n    \\'b3\\': tf.Variable(tf.random_normal([n_hidden_3])),\\n    \\'b4\\': tf.Variable(tf.random_normal([n_hidden_4])),\\n    \\'b5\\': tf.Variable(tf.random_normal([n_hidden_5])),\\n\\t\\'out2\\': tf.Variable(tf.random_normal([n_classes]))\\n}\\n'"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "#5 layer architecture\n",
    "X = tf.placeholder(\"float\", [None, n_input])\n",
    "Y = tf.placeholder(\"float\", [None, n_classes])\n",
    "#print(n_hidden_1)\n",
    "print(Y)\n",
    "# Define weights and biases in Tensorflow according to the parameters set above\n",
    "n_hidden_1 = 35  # 1st layer number of neurons\n",
    "n_hidden_2 = 30\n",
    "n_hidden_3 = 25\n",
    "n_hidden_4 = 20\n",
    "n_hidden_5 = 15\n",
    "weights = {\n",
    "\t'h1': tf.Variable(tf.random_normal([n_input, n_hidden_1])),\n",
    "    'h2': tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2])),\n",
    "    'h3': tf.Variable(tf.random_normal([n_hidden_2, n_hidden_3])),\n",
    "    'h4': tf.Variable(tf.random_normal([n_hidden_3, n_hidden_4])),\n",
    "    'h5': tf.Variable(tf.random_normal([n_hidden_4, n_hidden_5])),\n",
    "\t'out1': tf.Variable(tf.random_normal([n_hidden_5, n_classes]))\n",
    "}\n",
    "biases = {\n",
    "\t'b1': tf.Variable(tf.random_normal([n_hidden_1])),\n",
    "\t'b2': tf.Variable(tf.random_normal([n_hidden_2])),\n",
    "    'b3': tf.Variable(tf.random_normal([n_hidden_3])),\n",
    "    'b4': tf.Variable(tf.random_normal([n_hidden_4])),\n",
    "    'b5': tf.Variable(tf.random_normal([n_hidden_5])),\n",
    "\t'out2': tf.Variable(tf.random_normal([n_classes]))\n",
    "}\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n#3 layer architecture\\nX = tf.placeholder(\"float\", [None, n_input])\\nY = tf.placeholder(\"float\", [None, n_classes])\\n#print(n_hidden_1)\\nprint(Y)\\n# Define weights and biases in Tensorflow according to the parameters set above\\nn_hidden_1 = 50  # 1st layer number of neurons\\nn_hidden_2 = 40\\nweights = {\\n\\t\\'h1\\': tf.Variable(tf.random_normal([n_input, n_hidden_1])),\\n    \\'h2\\': tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2])),\\n    \\'h3\\': tf.Variable(tf.random_normal([n_hidden_2, n_hidden_3])),\\n\\t\\'out1\\': tf.Variable(tf.random_normal([n_hidden_3, n_classes]))\\n}\\nbiases = {\\n\\t\\'b1\\': tf.Variable(tf.random_normal([n_hidden_1])),\\n\\t\\'b2\\': tf.Variable(tf.random_normal([n_hidden_2])),\\n    \\'b3\\': tf.Variable(tf.random_normal([n_hidden_3])),\\n\\t\\'out2\\': tf.Variable(tf.random_normal([n_classes]))\\n}\\nprint(weights[\\'h1\\'])\\n#print(weights[\\'h2\\'])\\nprint(weights[\\'out1\\'])\\n'"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "#3 layer architecture\n",
    "X = tf.placeholder(\"float\", [None, n_input])\n",
    "Y = tf.placeholder(\"float\", [None, n_classes])\n",
    "#print(n_hidden_1)\n",
    "print(Y)\n",
    "# Define weights and biases in Tensorflow according to the parameters set above\n",
    "n_hidden_1 = 50  # 1st layer number of neurons\n",
    "n_hidden_2 = 40\n",
    "weights = {\n",
    "\t'h1': tf.Variable(tf.random_normal([n_input, n_hidden_1])),\n",
    "    'h2': tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2])),\n",
    "    'h3': tf.Variable(tf.random_normal([n_hidden_2, n_hidden_3])),\n",
    "\t'out1': tf.Variable(tf.random_normal([n_hidden_3, n_classes]))\n",
    "}\n",
    "biases = {\n",
    "\t'b1': tf.Variable(tf.random_normal([n_hidden_1])),\n",
    "\t'b2': tf.Variable(tf.random_normal([n_hidden_2])),\n",
    "    'b3': tf.Variable(tf.random_normal([n_hidden_3])),\n",
    "\t'out2': tf.Variable(tf.random_normal([n_classes]))\n",
    "}\n",
    "print(weights['h1'])\n",
    "#print(weights['h2'])\n",
    "print(weights['out1'])\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#1 layer architecture\n",
    "def multilayer_perceptron(x):\n",
    "    # define the layers of a single layer perceptron\n",
    "    layer_1 = tf.add(tf.matmul(x, weights['h1']), biases['b1'])\n",
    "    out_layer = tf.nn.sigmoid(tf.matmul(layer_1, weights['out1']) + biases['out2'])\n",
    "    return out_layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n#2 layer architecture\\ndef multilayer_perceptron(x):\\n    # define the layers of a single layer perceptron\\n    layer_1 = tf.add(tf.matmul(x, weights['h1']), biases['b1'])\\n    layer_2 = tf.add(tf.matmul(layer_1, weights['h2']) ,biases['b2'])\\n    out_layer = tf.nn.sigmoid(tf.matmul(layer_2, weights['out1']) + biases['out2'])\\n    return out_layer\\n\""
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "#2 layer architecture\n",
    "def multilayer_perceptron(x):\n",
    "    # define the layers of a single layer perceptron\n",
    "    layer_1 = tf.add(tf.matmul(x, weights['h1']), biases['b1'])\n",
    "    layer_2 = tf.add(tf.matmul(layer_1, weights['h2']) ,biases['b2'])\n",
    "    out_layer = tf.nn.sigmoid(tf.matmul(layer_2, weights['out1']) + biases['out2'])\n",
    "    return out_layer\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n#3 layer architecture\\ndef multilayer_perceptron(x):\\n    # define the layers of a single layer perceptron\\n    layer_1 = tf.add(tf.matmul(x, weights['h1']), biases['b1'])\\n    layer_2 = tf.add(tf.matmul(layer_1, weights['h2']) ,biases['b2'])\\n    layer_3 = tf.add(tf.matmul(layer_2, weights['h3']) ,biases['b3'])\\n    out_layer = tf.nn.sigmoid(tf.matmul(layer_3, weights['out1']) + biases['out2'])\\n    return out_layer\\n\""
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "#3 layer architecture\n",
    "def multilayer_perceptron(x):\n",
    "    # define the layers of a single layer perceptron\n",
    "    layer_1 = tf.add(tf.matmul(x, weights['h1']), biases['b1'])\n",
    "    layer_2 = tf.add(tf.matmul(layer_1, weights['h2']) ,biases['b2'])\n",
    "    layer_3 = tf.add(tf.matmul(layer_2, weights['h3']) ,biases['b3'])\n",
    "    out_layer = tf.nn.sigmoid(tf.matmul(layer_3, weights['out1']) + biases['out2'])\n",
    "    return out_layer\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n#5 layer architecture\\ndef multilayer_perceptron(x):\\n    # define the layers of a single layer perceptron\\n    layer_1 = tf.add(tf.matmul(x, weights['h1']), biases['b1'])\\n    layer_2 = tf.add(tf.matmul(layer_1, weights['h2']) ,biases['b2'])\\n    layer_3 = tf.add(tf.matmul(layer_2, weights['h3']) ,biases['b3'])\\n    layer_4 = tf.add(tf.matmul(layer_3, weights['h4']) ,biases['b4'])\\n    layer_5 = tf.add(tf.matmul(layer_4, weights['h5']) ,biases['b5'])\\n    out_layer = tf.nn.sigmoid(tf.matmul(layer_5, weights['out1']) + biases['out2'])\\n    return out_layer\\n\""
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "#5 layer architecture\n",
    "def multilayer_perceptron(x):\n",
    "    # define the layers of a single layer perceptron\n",
    "    layer_1 = tf.add(tf.matmul(x, weights['h1']), biases['b1'])\n",
    "    layer_2 = tf.add(tf.matmul(layer_1, weights['h2']) ,biases['b2'])\n",
    "    layer_3 = tf.add(tf.matmul(layer_2, weights['h3']) ,biases['b3'])\n",
    "    layer_4 = tf.add(tf.matmul(layer_3, weights['h4']) ,biases['b4'])\n",
    "    layer_5 = tf.add(tf.matmul(layer_4, weights['h5']) ,biases['b5'])\n",
    "    out_layer = tf.nn.sigmoid(tf.matmul(layer_5, weights['out1']) + biases['out2'])\n",
    "    return out_layer\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = multilayer_perceptron(X)\n",
    "# Define loss(softmax_cross_entropy_with_logits) and optimizer(AdamOptimizer)\n",
    "loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits, labels=Y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "train_op = optimizer.minimize(loss_op)\n",
    "# Initializing the variables\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for some macosx installations, conflicting copies of mpilib causes trouble with tensorflow.\n",
    "# use the following two lines to resolve that issue\n",
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train acc: 0.568958, Test_acc: 0.603750\n",
      "Train acc: 0.712375, Test_acc: 0.711250\n",
      "Train acc: 0.786125, Test_acc: 0.768750\n",
      "Train acc: 0.823375, Test_acc: 0.787500\n",
      "Train acc: 0.850625, Test_acc: 0.816250\n",
      "Train acc: 0.868167, Test_acc: 0.830000\n",
      "Train acc: 0.877667, Test_acc: 0.840000\n",
      "Train acc: 0.884583, Test_acc: 0.842500\n",
      "Train acc: 0.889542, Test_acc: 0.851250\n",
      "Train acc: 0.894917, Test_acc: 0.848750\n",
      "Train acc: 0.898375, Test_acc: 0.843750\n",
      "Train acc: 0.901625, Test_acc: 0.850000\n",
      "Train acc: 0.905750, Test_acc: 0.858750\n",
      "Train acc: 0.908667, Test_acc: 0.850000\n",
      "Train acc: 0.911917, Test_acc: 0.857500\n",
      "Train acc: 0.912417, Test_acc: 0.848750\n",
      "Train acc: 0.915958, Test_acc: 0.856250\n",
      "Train acc: 0.916167, Test_acc: 0.851250\n",
      "Train acc: 0.917625, Test_acc: 0.852500\n",
      "Train acc: 0.920458, Test_acc: 0.853750\n",
      "Train acc: 0.920500, Test_acc: 0.858750\n",
      "Train acc: 0.923292, Test_acc: 0.852500\n",
      "Train acc: 0.922000, Test_acc: 0.846250\n",
      "Train acc: 0.920792, Test_acc: 0.827500\n",
      "Train acc: 0.918083, Test_acc: 0.831250\n",
      "Train acc: 0.920167, Test_acc: 0.827500\n",
      "Train acc: 0.920125, Test_acc: 0.825000\n",
      "Train acc: 0.925083, Test_acc: 0.841250\n",
      "Train acc: 0.924167, Test_acc: 0.845000\n",
      "Train acc: 0.927250, Test_acc: 0.847500\n",
      "Train acc: 0.925625, Test_acc: 0.851250\n",
      "Train acc: 0.928292, Test_acc: 0.853750\n",
      "Train acc: 0.928708, Test_acc: 0.853750\n",
      "Train acc: 0.929000, Test_acc: 0.856250\n",
      "Train acc: 0.930875, Test_acc: 0.851250\n",
      "Train acc: 0.931458, Test_acc: 0.858750\n",
      "Train acc: 0.930083, Test_acc: 0.845000\n",
      "Train acc: 0.930917, Test_acc: 0.843750\n",
      "Train acc: 0.933833, Test_acc: 0.842500\n",
      "Train acc: 0.932042, Test_acc: 0.847500\n",
      "Train acc: 0.932708, Test_acc: 0.850000\n",
      "Train acc: 0.935333, Test_acc: 0.851250\n",
      "Train acc: 0.934500, Test_acc: 0.851250\n",
      "Train acc: 0.938958, Test_acc: 0.855000\n",
      "Train acc: 0.940292, Test_acc: 0.856250\n",
      "Train acc: 0.941208, Test_acc: 0.855000\n",
      "Train acc: 0.941500, Test_acc: 0.852500\n",
      "Train acc: 0.940542, Test_acc: 0.848750\n",
      "Train acc: 0.940708, Test_acc: 0.853750\n",
      "Train acc: 0.941542, Test_acc: 0.857500\n",
      "Time elapsed - 14.556270122528076 seconds.\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    start_time = time.time()\n",
    "    sess.run(init)\n",
    "\n",
    "    h = h5py.File('data1.hdf5', 'r')\n",
    "    n1 = h.get('data_train') \n",
    "    n2 = h.get('data_test')\n",
    "\n",
    "    # Training cycle\n",
    "    total_batch_train = int(n1.shape[0] / batch_size)\n",
    "    total_batch_test = int(n2.shape[0] / batch_size)\n",
    "\n",
    "    for iter_num in range(num_epochs):\n",
    "        # variables for train and test accuracies\n",
    "        avg_acc_train = 0.\n",
    "        avg_acc_test = 0.\n",
    "        for i in range(total_batch_train):\n",
    "            train_x = n1[(i) * batch_size: (i + 1) * batch_size, ...]\n",
    "            train_y = labels_train[(i) * batch_size: (i + 1) * batch_size, :]\n",
    "\n",
    "            _, c_train, _logits_train = sess.run([train_op, loss_op, logits], feed_dict={X: train_x, Y: train_y})\n",
    "            _label_train = [np.argmax(i) for i in _logits_train]\n",
    "            _label_train_y = [np.argmax(i) for i in train_y]\n",
    "            _accuracy_train = np.mean(np.array(_label_train) == np.array(_label_train_y))\n",
    "            avg_acc_train += _accuracy_train\n",
    "\n",
    "\n",
    "        for j in range(total_batch_test):\n",
    "            test_x = n2[(j) * batch_size: (j + 1) * batch_size, ...]\n",
    "            test_y = labels_test[(j) * batch_size: (j + 1) * batch_size, :]\n",
    "\n",
    "            c_test, _logits_test = sess.run([loss_op, logits], feed_dict={X: test_x, Y: test_y})\n",
    "            _label_test = [np.argmax(i) for i in _logits_test]\n",
    "            _label_test_y = [np.argmax(i) for i in test_y]\n",
    "            _accuracy_test = np.mean(np.array(_label_test) == np.array(_label_test_y))\n",
    "            avg_acc_test += _accuracy_test\n",
    "\n",
    "        # print the train and test accuracies   \n",
    "        print(\"Train acc: %f, Test_acc: %f\" % (avg_acc_train/total_batch_train, avg_acc_test/total_batch_test))\n",
    "    duration = time.time() - start_time\n",
    "    print('Time elapsed - {} seconds.'.format(duration))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
